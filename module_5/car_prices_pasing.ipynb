{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries that will be used in the project\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# PART I\n",
    "collect advertises urls from searching pages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    FUNCTION\n",
    "    Collection all car urls for a specified brand and model from the `auto.ru` website.\n",
    "    For each brand and model the number of available pages is calculated and the urls\n",
    "    from all these pages are saved into a list.\n",
    "\"\"\"\n",
    "\n",
    "def collect_car_urls(\n",
    "        brand: str,\n",
    "        model: str\n",
    "):  # sad smile\n",
    "    \"\"\"\n",
    "    :param brand: brand name\n",
    "    :paran model: model name\n",
    "    :return: list urls from all web pages\n",
    "    \"\"\"\n",
    "\n",
    "    # constant:\n",
    "    # tail of url: ask to order of view as table\n",
    "    TAIL='&output_type=table/'\n",
    "\n",
    "    page = 1\n",
    "\n",
    "    # url, get, soup\n",
    "    main_url = f'https://auto.ru/cars/{brand}/{model}/all/?sort=price-desc&page='\n",
    "    # urls only used cars\n",
    "    # main_url = f'https://auto.ru/cars/{brand}/{model}/used/?sort=price-desc&page='\n",
    "    # urls only used cars for the last 7 days (page filters has also 1, 2, 3, 14, 21, 31 days)\n",
    "    # main_url = f'https://auto.ru/cars/{brand}/{model}/used/?sort=price-desc&top_days=7&page=\n",
    "    main_response = requests.get(main_url+str(page)+TAIL)\n",
    "    main_soap = BeautifulSoup(main_response.content.decode('utf-8'), 'html.parser')\n",
    "    # delete unbreakable space from text\n",
    "\n",
    "    # find data from page to calculate # of pages.\n",
    "    try:\n",
    "        _ = main_soap.find('span', class_='ButtonWithLoader__content').text.replace(u'\\xa0', '')\n",
    "\n",
    "        # calculate total page number of specified brand\n",
    "        urls_total = int(re.findall(r'\\d+', _)[0])\n",
    "        ads_per_page = len(main_soap.find_all('a', class_='Link ListingItemTitle__link'))\n",
    "        pages_num = urls_total // ads_per_page + 1\n",
    "\n",
    "    # if no data - return message about it\n",
    "    except:\n",
    "        print(f'IMPORTANT. There is no data for {brand} {model}.')\n",
    "        return []\n",
    "\n",
    "    print(f\"Total pages for {brand} {model} is {pages_num}.\")\n",
    "\n",
    "    # prepare var for collect urls\n",
    "    all_urls = []\n",
    "\n",
    "    # cycle to collect urls from search page\n",
    "    for page_num in range(1, pages_num):\n",
    "\n",
    "        # print step of processing (every 10 pages), you can make another step\n",
    "        if page_num % 10 == 0:\n",
    "            print(f\"...Extracting page {page_num} from {pages_num}.\")\n",
    "\n",
    "        # url, get, soup\n",
    "\n",
    "        page_response = requests.get(main_url+str(page_num)+TAIL)\n",
    "        time.sleep(0.1)                             # sleep to avoid CAPTCHA or ban\n",
    "        page_soap = BeautifulSoup(page_response.content.decode('utf-8'), 'html.parser')\n",
    "\n",
    "        # collecting all advertisement's urls\n",
    "        all_urls.extend([a.get('href') for a in page_soap.find_all('a', class_='Link ListingItemTitle__link')])\n",
    "\n",
    "    return all_urls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Main cycle\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select brands and models, which we will need to collect, as they are in test dataset\n",
    "# take it from test dataset.\n",
    "models = list(test[['brand', 'model_name']].\n",
    "              groupby(['brand', 'model_name'])['model_name'].\n",
    "              count().\n",
    "              to_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make some variables to collect data and create empty file\n",
    "urls = []\n",
    "list_urls = pd.DataFrame({'car_url': urls})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_urls.to_csv('list_urls.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ! WARNING ! Collecting data is long process.\n",
    "    -------------------------------------------\n",
    "    every 20 pages takes approximately 55 sec.\n",
    "\"\"\"\n",
    "\n",
    "for brand, model in models:\n",
    "\n",
    "    # printing a message about the current brand-model\n",
    "    print(f\"Extracting data for the brand {brand} {model}:\")\n",
    "\n",
    "    # collecting all urls from all pages with specified brand\n",
    "    urls = collect_car_urls(brand, model)\n",
    "\n",
    "    # saving records to the file\n",
    "    list_urls = pd.DataFrame({'car_url': urls})\n",
    "    list_urls.to_csv('list_urls.csv', index=False, header=False, mode='a')\n",
    "    print(f\"Extracted data for the {brand} {model} saved to file\")\n",
    "    print('-----------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# RESULT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# total parsed urls\n",
    "df = pd.read_csv('list_urls.csv')\n",
    "len(df)\n",
    "# 122_472"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# but we have some duplicates\n",
    "df[df.duplicated()].shape[0]\n",
    "# 558\n",
    "\n",
    "# drop them\n",
    "df.drop_duplicates(inplace=True)\n",
    "len(df)\n",
    "# 121_914"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# collecting urls contain urls of new auto, but webpages of new cars as it turned out,\n",
    "# have different structure and my function can't collect all necessary data.\n",
    "# So we need to filter out only used car.\n",
    "df['brand']=df.car_url.apply(lambda q: q.split('/')[6])\n",
    "df['model']=df.car_url.apply(lambda q: q.split('/')[7])\n",
    "df['novice']=df.car_url.apply(lambda q: q.split('/')[4])\n",
    "df_used = df[df.novice == 'used']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_urls = df_used.car_url.to_list()\n",
    "len(all_urls)\n",
    "# 99_096"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# PART II\n",
    "Get information about every cars from theres web pages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a list of aimed features\n",
    "features = test.columns.to_list() + ['price']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    For the specified url extract information from the `auto.ru` webpage about all necessary features mentioning in the extracted_columns list, such as model_name, mileage, price, etc.\n",
    "    The function returns the list of values for all specified features in the same order as its fields are presented in the extracted_columns list.\n",
    "\"\"\"\n",
    "def extract_url_data(url: str):\n",
    "    \"\"\"\n",
    "    :param url: webpage url\n",
    "    :return: list of values in order as fields are in test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    global features\n",
    "\n",
    "    response = requests.get(url)\n",
    "    page = BeautifulSoup(response.content.decode('utf-8'), 'html.parser')\n",
    "\n",
    "    print(\"*\")\n",
    "    # main set of the features\n",
    "    try:\n",
    "        data = json.loads(page.find('script', type=\"application/ld+json\").string)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # features \"url\", \"priceCurrency\" and \"price\"\n",
    "    # will take them from the nested dictionary \"offers\"\n",
    "    try:\n",
    "        off = data['offers']\n",
    "        data['car_url'],data['priceCurrency'],data['price'] = off['url'],off['priceCurrency'],off['price']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"mileage\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        card = page.find(\n",
    "            'li', class_='CardInfoRow CardInfoRow_kmAge').find_all('span')[-1].text.replace(u'\\xa0', u'')\n",
    "        data['mileage'] = int(re.findall(r'\\d+', card)[0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"bodyType\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        card = page.find('li', class_='CardInfoRow CardInfoRow_bodytype').find_all('span')[-1].text\n",
    "        data['bodyType'] = card\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # print(\"**\")\n",
    "    # feature \"model_name\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        data['model_name'] = page.find_all(\n",
    "            'div', class_='InfoPopup InfoPopup_theme_plain InfoPopup_withChildren BreadcrumbsPopup')[1].text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"parsing_unixtime\"\n",
    "    # calculating it from real time of parsing\n",
    "    try:\n",
    "        data['parsing_unixtime'] = int(time.time())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"sell_id\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        data['sell_id'] = int(re.findall(\n",
    "            r'\\d+', page.find('div', class_='CardHead__infoItem CardHead__id').text)[0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"super_gen\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        data['super_gen'] = json.loads(page.find('div', id=\"sale-data-attributes\").get('data-bem'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # print('***')\n",
    "    # feature \"Владельцы\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        data['Владельцы'] = page.find(\n",
    "            'li', class_='CardInfoRow CardInfoRow_ownersCount').find_all('span')[-1].text.replace(u'\\xa0', u' ')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"Владение\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        data['Владение'] = page.find(\n",
    "            'li', class_='CardInfoRow CardInfoRow_owningTime').find_all('span')[-1].text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"ПТС\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        data['ПТС'] = page.find(\n",
    "            'li', class_='CardInfoRow CardInfoRow_pts').find_all('span')[-1].text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"Привод\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        data['Привод'] = page.find(\n",
    "            'li', class_='CardInfoRow CardInfoRow_drive').find_all('span')[-1].text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"Руль\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        data['Руль'] = page.find('li', class_='CardInfoRow CardInfoRow_wheel').find_all('span')[-1].text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"Состояние\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        data['Состояние'] = page.find(\n",
    "            'li', class_='CardInfoRow CardInfoRow_state').find_all('span')[-1].text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"Таможня\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        data['Таможня'] = page.find(\n",
    "            'li', class_='CardInfoRow CardInfoRow_customs').find_all('span')[-1].text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"description\"\n",
    "    # replacing some noise with spaces in values\n",
    "    try:\n",
    "        data['description'] = re.sub('\\W+', ' ', data['description'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #print('** **')\n",
    "    # feature \"vehicleTransmission\"\n",
    "    # taking it from page\n",
    "    try:\n",
    "        card = page.find('li', class_=\"CardInfoRow CardInfoRow_transmission\").find_all('span')[-1].text\n",
    "        data['vehicleTransmission'] = card\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"engineDisplacement\", \"enginePower\", \"fuelType\"\n",
    "    # taking them from page\n",
    "    try:\n",
    "        card = page.find('li', class_=\"CardInfoRow CardInfoRow_engine\").find_all('span')[-1].text\n",
    "        card = card.replace(u'\\xa0', u' ').split(' / ')\n",
    "        data['engineDisplacement'], data['enginePower'], data['fuelType'] = card\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #print('*** **')\n",
    "    # feature \"complectation_dict\"\n",
    "    # will take the dict from page\n",
    "    try:\n",
    "        catalog_url = page.find('a', class_='Link SpoilerLink CardCatalogLink SpoilerLink_type_default').get('href')\n",
    "        response_catalog = requests.get(catalog_url)\n",
    "        page_catalog = BeautifulSoup(response_catalog.content.decode('utf-8'), 'html.parser')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        json_data_catalog = json.loads(\n",
    "            page_catalog.find('script', type=\"application/json\", id='initial-state').string)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        data['complectation_dict'] = json_data_catalog['state']['compare']['selected'][0]['specifications']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # feature \"numberOfDoors\"\n",
    "    # taking it from the \"complectation dict\"\n",
    "    try:\n",
    "        data['numberOfDoors'] = json_data_catalog['state']['compare']['selected'][0]['specifications']['doors-count']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print('*** ***')\n",
    "    # feature \"equipment_dict\"\n",
    "    # will take the dict from page\n",
    "    try:\n",
    "        json_data_equip = json.loads(\n",
    "            page.find('script', type=\"application/json\", id='initial-state').string)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        data['equipment_dict'] = json_data_equip['card']['vehicle_info']['equipment']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # making a list with data from webpage in order of columns of test dataset\n",
    "    output = []\n",
    "    try:\n",
    "        for col in features:\n",
    "            output.append(data.get(col, None))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # two features are not found in webpage: model_info, vendor\n",
    "\n",
    "    if not output:\n",
    "        output = [None] * len(features)\n",
    "\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Main cycle (PART II)\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initiate variables to collect data\n",
    "final_list = []\n",
    "final_df = pd.DataFrame(columns=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create file to add data.\n",
    "\"\"\" WARNING\n",
    "    If you started after interrupt - do NOT execute this block of script:\n",
    "    it just clean up already collected data\n",
    "\"\"\"\n",
    "final_df.to_csv('train.csv', index=False, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ! WARNING ! Collecting data is long process.\n",
    "    -------------------------------------------\n",
    "    approximately 50 records takes 90-100 sec.\n",
    "\"\"\"\n",
    "\n",
    "final_list = []\n",
    "for n, url in enumerate(all_urls):\n",
    "    # printing a message about the current status of the program execution\n",
    "    if n % 50 == 0:\n",
    "        print(f\"The # of current processing URL is {n}, url is {url}.\")\n",
    "    # printing a message about reaching 1000 records for saving to a file\n",
    "    if n % 1000 == 0:\n",
    "\n",
    "        # make dataframe from list of collected data and add it to csv-file\n",
    "        final_df = pd.DataFrame(data=final_list, columns=features)\n",
    "        final_df.to_csv('train.csv', index=False, header=False, mode='a')\n",
    "        print(f\"Collected data was add to csv-file.\")\n",
    "\n",
    "        final_list = []\n",
    "    final_list.append(extract_url_data(url))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Some helpful script\n",
    "1. add collected data when proces was interrupted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if process was interrupted, we can ad collected data to file\n",
    "final_df = pd.DataFrame(data=final_list, columns=features)\n",
    "print(f'Add to file: {final_df.shape[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df.to_csv('train.csv', index=False, header=False, mode='a')\n",
    "final_df = pd.DataFrame(columns=features)\n",
    "final_list=[]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. calculate urls index, to start again after interrupt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "df = pd.read_csv('list_urls.csv')\n",
    "df.drop_duplicates(inplace=True)\n",
    "#\n",
    "#df['novice']=df.car_url.apply(lambda q: q.split('/')[4])\n",
    "#df_used = df[df.novice == 'used']\n",
    "\n",
    "# making list with urls\n",
    "all_urls = df_used.car_url.to_list()\n",
    "# find index of last record url from train\n",
    "all_urls.index(train.car_url.iloc[-1])\n",
    "\n",
    "# the founded index must be esed in main cycle above as:\n",
    "# for n, url in enumerate(all_urls[index+1:]):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# RESULT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "print(f'Train data shape : {train.shape},\\n test data shape : {test.shape}')\n",
    "# Train data shape : (103794, 33),\n",
    "#  test data shape : (34686, 33)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}