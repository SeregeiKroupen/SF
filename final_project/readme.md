# Финальный проект курса Data Science
тема проекта:   
## "Предсказание цены домов для нахождения выгодных предложений для инвестиций"  

Первоначальный план был в целом стандартный и цель была скромнее:
1) Провести очистку, анализ и обработку данных - EDA
2) Применить ML-модели для прогнозирования
3) Применить нейросеть для прогнозирования

Однако, в процессе анализа датафрейма, выяснилось, что у большого количества записей 
имеется информация о цене за квадратный фут и площадь самой недвижимости. Таким образом, 
вышло, что нужды в "предсказании" цены нет. И мне пришла идея в голову, что можно 
построить рабочую модель по поиску выгодных предложений о продаже, чтобы рассмотреть
возможности и инвестирования. То есть, найти предложения с заниженной ценой.  
Другая идея, которую я захотел попробовать - эта идея предварительной кластериции датасета, 
перед применением регрессионной модели, причем для каждого кластера раздельной. Я уже пытался 
ранне применить эдту схему в предыдущем проекте, но значимых результатов не получил.  

Таким образом, итоговый план стал следующий:
1) EDA & FE
2) классический ML разными моделями
3) кластеризация датасета в ручную и ML регрессия внутри кластеров
4) ML-кластеризация датасета и ML регрессия внутри кластеров
5) построение нейросети
6) применение нейросети внутри кластеров
7) предложение механизма применеия на практике - production

## Ноутбуки и данные:
п.1 - Final_project_EDA.ipynb  
п.2-4,7 - Final_project_ML.ipynb  
п.5-6 - Final_project[ML+NN] (Kaggle)  
https://www.kaggle.com/code/sergeikroupen/final-project-ml-nn/edit  
На Kaggle выложен и предобработанный датадфрейм после EDA&FE фазы  
https://www.kaggle.com/datasets/sergeikroupen/housing-preprocessed-data

## EDA & FE
На этапе обработки исходных данных выснились две очень не приятные вещи.  

Первая - в датафрейме очень много дублей записей. По моей оценке - от 1/3 до 2/5 всего
объема. Под дублями записей я понимаю комбинацию
полного адреса (дом+улица+город+штат). Причем, стандартными методам pd.DataFrame.drop()
очистить не удастся, так как в записях присутствуют ошибки, разное написание, лишние 
символы, перестановка - полный набор "шума", работа с которым очень трудоемка. Кроме того,
дважды подбираясь к этой теме, удаляя часть дублей я необъяснимым образом для себя
сталкивался с ухудшением метрик ML-модели. Таким образом, я бросил эту затею.

Вторая неприятность - огромное количество ошибок и неточностей как в сопуствующей информации,
 вроде "наличия бассейна", так и в критической - вроде площади недвижимости. Я много раз
находил эти ошибки, просто загуглив адреса из датафрейма. Причем, что самое грустное, 
среди дублей, как правило была только одна запись с полностью верной информацией, тогда как
остальные содержали ее не полную или искаженную версию. Думаю, именно по-этому "простое" (на
самом деле нет) удаление дублей не улучало метрику - корректная информация в основном 
терялась.

Прежде всего, очень важным виделся вопрос выбора метрики. Пока шла очистка и генерация признаков,
я просматривал широкий набор. Но, учитывая выбранную цель, нужна была метрика, отслеживающая, 
чтобы модель преймущественно ошибалась "вниз", то есть, предсказывала меньшее значение, 
чем реальная цена предложения. Это необходимо для страховки от ложного вывода, когда
предсказание модели окажется выше рыночной цены и объект окажется плохим для инвестиций.
Я нешел метрику Mean Pinball Loss - "метрика средних потерь в опредленном квантиле". 
Она реализована в sklearn, но почему то у меня локально не импортировалась. Так что я 
написал собственную функцию с дополнительнвыми результирующими параметрами: кроме MPL я 
еще считал долю "преоценненых" занчений, а также среднее по "переоценным" и "недооцененным"
значениям. Выбрав парметр альфа=0.01, я сфокусировался на минимизации "переоценного"
значения.

Формула:  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtext>pinball</mtext>
  <mo stretchy="false">(</mo>
  <mi>y</mi>
  <mo>,</mo>
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mi>y</mi>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mn>1</mn>
    <msub>
      <mi>n</mi>
      <mrow data-mjx-texclass="ORD">
        <mtext>samples</mtext>
      </mrow>
    </msub>
  </mfrac>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>0</mn>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mi>n</mi>
        <mrow data-mjx-texclass="ORD">
          <mtext>samples</mtext>
        </mrow>
      </msub>
      <mo>&#x2212;</mo>
      <mn>1</mn>
    </mrow>
  </munderover>
  <mi>&#x3B1;</mi>
  <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
  <mo stretchy="false">(</mo>
  <msub>
    <mi>y</mi>
    <mi>i</mi>
  </msub>
  <mo>&#x2212;</mo>
  <msub>
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>y</mi>
        <mo stretchy="false">^</mo>
      </mover>
    </mrow>
    <mi>i</mi>
  </msub>
  <mo>,</mo>
  <mn>0</mn>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <mo stretchy="false">(</mo>
  <mn>1</mn>
  <mo>&#x2212;</mo>
  <mi>&#x3B1;</mi>
  <mo stretchy="false">)</mo>
  <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
  <mo stretchy="false">(</mo>
  <msub>
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>y</mi>
        <mo stretchy="false">^</mo>
      </mover>
    </mrow>
    <mi>i</mi>
  </msub>
  <mo>&#x2212;</mo>
  <msub>
    <mi>y</mi>
    <mi>i</mi>
  </msub>
  <mo>,</mo>
  <mn>0</mn>
  <mo stretchy="false">)</mo>
</math>
______

Анализ целевого вектора - цены - показал крайне широкий диапазон значений: фактически от 
нуля до 200 миллионов долларов. Осознание этого факта и привело меня к мысли обязательно
попробовать кластеризацию, так как не возможно построить модель с приемлемыми занчениями
точности на таком диапазоне колебания цен. Кроме того, существет очевидно кластер 
недвижимости (очень дорогой), цена на которую нетривиально зависит от многих прочих факторов,
помимо простого пречисления ванных комнат, бассейна или даже места на карте.

В целом я применял "щадящий" подход к векторам. То есть, я извлекая из векторов информацию
(числовые данные, классификационную информацию),
как правило, не удалял его (только в стадии моделирования - просто исключал), чтобы уже в конце 
оценивать важность каждого вектора, несмотря на очевидное дублирование данных.

В процессе работы вынужденно решал удалять часть записей из датафрейма. 
+ записи, где указано, что это план застройки, домов еще нет (~2.5К)
+ записи без указания адреса (как же потом найти их?) (~1.7К)
+ записи с явным указание на продажу земли, а не домов (~31К)
+ записи полные дубликаты (адрес+площадь+цена) (~24К)
+ записи с нулями в графах "площадь" и "площадь лота с землей" (~14К)   

Как раз после очередного удаления таких записей метрика МАРЕ резко улучшилась.

В очередной раз убедился, что для работы с данными, нужно глубоко разбираться в предмете,
который эти данные характеризуют. Простой пример знания, которое я почерпнул, выясняя,
что за цифры записаны в векторе "ванные комнаты". Там встречались цифры 2.5, 2.75, 2.1.
Оказалось, что эта особенность, которую у нас не встретишь. "Ванной комнатой" американцы 
считают комнату с душем, ванной, умывальником и туалетом. Ванные комнаты с не полным 
набором считалются "неполной ванной комнатой". Далее, есть два подхода к отражению
этой информации. Старый подход считал целые ванные команты за целые числа, а не полные - 
за полвинки. Так получаются числа в роде 1.5, 2.5 и т.д. Вариант этого подхода: учет
того сколько из четырех ввещей присутствует в ванной, чтобы столько четвертей и указываать для 
неполной ванны. Этот подход реализует варианты 2.75 или 3.25, например. Увы, часто в
таком подходе цифры ванных просто складывают, и получается совсем искаженная картина:
1 + 0.5 + 0.75 = 2.25 - понять по цифре, сколько и каких ванных в доме не представляется 
возможным - можно только гадать с вероятностью. Второй подход призывает отдельно считать
"полные" и "неполные" ванные комнаты и записывать их через точку: 3.2, 2.1. Этот подход, 
по моему, самый прозрачный и понятный. Но увы, привести все данные к нему в датафреймие
не представляется возможным. По-этому, я реализовал округление в большую сторону, как
некий компромисс.

## ML

ML моделирование не принесло неожиданностей, и лучшей моделью оказался CatBoost Regression.
MAPE=4.09%, Pinball = 17 168

Кластеризация датафрейма в ручную была произведена по целевому вектору с дипазонами по
500 000 долларов, 1 500 000 долларов и в конце диапазоны до 10 млн, 20, млн и 100 млн.
также я сознательно исключил записи до 50 000 долларов, так как по цене это скорее всего
не недвижимость а вариант ренты или земля, не исключенная по явным признакам.
В результате этой операции уже удалось получить значения метрики лучше, чем в обычной схеме:   
для МАРЕ вплоть до 1.06% (в подавляющем большинстве - лучше 3%)  
для Pinball - даже еще лучше, "переоценка" составляла в среднем менее 2% от цены в кластере.

Но самые лучшие результаты получились после ML-кластеризации (алгритм  K-Means).
Предложив алгоритму произвести кластеризацию по 14 кластерам (столько же быдло на предыдущем
этапе). Так вышло, что в нескольких кластерах оказалось по мини мальному количеству записей - их 
по ходу дело пришлось исключать из дальнейшего моделирования, во избежания ошибок.
Я изначально решил применить два подхода к расчету предсказания цены:
+ расчтет средневзвешенного предсказания по всем моделям всех кластеров для каждой записи, где весами будет выступать обратные величины расстояний до центроидов ('price_sumprod') 
+ прямой расчет предсказания модели, расчитанной для кластера текущей записи ('price_kmeans')

В итоге вышло, что расчет метрики по всему Датафрейму по вектору "price_kmeans" дал
результат:  
МАРЕ = 2.13%, Pinball = 12 126  
Что оказалось лучше простого ML-моделирования. А анализ метрик внутри кластеров в некоторых
случаях ушел ниже 1% ошибки.  

Наверное, тут можно бы и остановится, так как я не представляю, как еще лучше сделать прогноз, 
если уже сейчас неточность всего 1%. Но я всетаки решил построить нейросеть и попытаться 
получить хотя бы такие же результаты.

 ## Нейросеть
Многослойные, многопараметральные нейросети показывали стабильно плохой результат. 
Сколько-нибудь "приличные" цифры получлись только у простой 2-3 слойной сети по 128-256 параметров
в слое. Но, покаазатель МАРЕ мне так и не удалось получить менее 100%. Сети также 
"отказывались" показывать стабильный результат, и уже через 1000 эпох перобучались.

Чnобы завершить эксперимент с нейросетью, я решил попробовать использовать кластеры, 
полученные на предыдущем этапе. И оказалось, что это сработало! Я попробовал только один 
кластер, но в нем даже со старта была получена стандартная картина динамики обучения 
(гипербола), дойдя в итоге после примерно 1000 эпох до уровня МАРЕ=3-5%. Это хороший
результат, который еще больше подтверждает правильность метода пердварииельной 
кластеризации данных перед меоделированием, хотя и оказался хуже результатов регрессионной 
модели. Но, лучше многих других регрессионных моделей, кроме CatBoost.

## Бизнес-производство
Итак, идея в том, чтобы выбрать кластер с интерсующей точностью (K-Means разбил на 
кластеры с широким диапазоном цен), получить список домов, где цена оказывается существенно 
ниже предсказанной - значит, что ее занизили для продажи, по сравнению с аналогичными 
предложениями.  
Я выбрал ТОП по дельте между ценой и предсказаниями. Вышли разумеется самые дорогие 
предложения. Для работы в этом сегметне нужен кредит на 60 млн.долларов.  
Но можно выбрать самые ходовые цены (до 750 тысяч). в этом случае для инвестиций 
понадобится 3.5 млн.долларов.  
Если исходить из предположения, что возврат инвестиций произойдет в течение года, то
доходность состаит 10-12% годовых (характерно для всех сегментов). Хотя по рынку видно, что оборот происходит быстрее,
и возврат средств может наступить в рамках квартала.


